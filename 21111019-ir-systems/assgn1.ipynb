{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f759be30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9449e08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# example to show working of stopwords and tokenization library in python\\nfile_content_temp = \"my Name is Ayush fir.st . second. .ninth\"  #note: period after a word is removed but period before a word is not removed\\ntokens_temp = nltk.word_tokenize(file_content_temp)\\nstop_words_temp = stopwords.words(\\'english\\')\\npunctuation_list_temp=[\\'\\'\\',\\'.\\',\\')\\',\\'(\\',\\'{\\',\\'}\\',\\'[\\',\\']\\',\\'\\\\,\\',\\'?\\',\\'!\\',\\'\\'\\'\\',\\':\\',\\';\\',\\'...\\']\\ntokens_temp = [word.lower() for word in tokens_temp if (word not in stop_words_temp and word not in punctuation_list_temp and word.isnumeric()==False)]\\nRESULT=[\\'name\\', \\'ayush\\', \\'fir.st\\', \\'second\\', \\'.ninth\\']\\n\\n#EXAMPLE TO SHOW WORKING OF ASCII REMOVAL IN PYTHON\\nstring_with_nonASCII = \"àa string withé fuünny charactersß.\"\\nencoded_string = string_with_nonASCII.encode(\"ascii\", \"ignore\")\\ndecode_string = encoded_string.decode()\\nprint(decode_string)\\nRESULT=\"a string with funny characters.\"\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# example to show working of stopwords and tokenization library in python\n",
    "file_content_temp = \"my Name is Ayush fir.st . second. .ninth\"  #note: period after a word is removed but period before a word is not removed\n",
    "tokens_temp = nltk.word_tokenize(file_content_temp)\n",
    "stop_words_temp = stopwords.words('english')\n",
    "punctuation_list_temp=['\\'','.',')','(','{','}','[',']','\\,','?','!','\\'\\'',':',';','...']\n",
    "tokens_temp = [word.lower() for word in tokens_temp if (word not in stop_words_temp and word not in punctuation_list_temp and word.isnumeric()==False)]\n",
    "RESULT=['name', 'ayush', 'fir.st', 'second', '.ninth']\n",
    "\n",
    "#EXAMPLE TO SHOW WORKING OF ASCII REMOVAL IN PYTHON\n",
    "string_with_nonASCII = \"àa string withé fuünny charactersß.\"\n",
    "encoded_string = string_with_nonASCII.encode(\"ascii\", \"ignore\")\n",
    "decode_string = encoded_string.decode()\n",
    "print(decode_string)\n",
    "RESULT=\"a string with funny characters.\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "93583808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7ec0a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2226.236476659775\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "documents={}\n",
    "start=time.time()\n",
    "stop_words = stopwords.words('english')     #stopwords in english langauge\n",
    "my_dict={}   # dictionary to store tokens and doc_id pairs\n",
    "#reading all the files in folder english-corpora\n",
    "curr_folder=os.path.join(os.getcwd(),\"english-corpora\")\n",
    "for root,folders,files in os.walk(curr_folder):\n",
    "    \n",
    "    for file in files:\n",
    "        path=os.path.join(root,file)\n",
    "        file_content= open(path, encoding='utf8').read()\n",
    "        \n",
    "        #fetch document id\n",
    "        doc_id=re.sub('.txt','',file)\n",
    "        \n",
    "\n",
    "        #tokenization\n",
    "        tokens = nltk.word_tokenize(file_content) \n",
    "        #remove punctuations\n",
    "        tokens=[word.translate(str.maketrans('','',string.punctuation)) for word in tokens]\n",
    "        #remove digits\n",
    "        tokens=[word.translate(str.maketrans('','','1234567890')) for word in tokens]\n",
    "        \n",
    "        tokens=[word for word in tokens if word!='']\n",
    "\n",
    "        tokens =[word.lower() for word in tokens if word not in stop_words]\n",
    "        \n",
    "\n",
    "        #removal of non-ascii characters\n",
    "        for i in range(0,len(tokens)):\n",
    "            temp=tokens[i].encode('ascii','ignore')\n",
    "            tokens[i]=temp.decode()\n",
    "\n",
    "        #stemming\n",
    "        ps=PorterStemmer()\n",
    "        tokens=[ps.stem(word) for word in tokens]\n",
    "              \n",
    "        for word in tokens:\n",
    "            if word in my_dict:\n",
    "                my_dict[word].add(doc_id)\n",
    "            else:\n",
    "                my_dict[word]=set()\n",
    "                my_dict[word].add(doc_id)\n",
    "            \n",
    "        documents[doc_id]=tokens\n",
    "        \n",
    "        \n",
    "        \n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90765e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dictionary to pickle file for fast loading \n",
    "import pickle\n",
    "\n",
    "#posting list dictionary : \n",
    "#file=open('tokens-dict.pkl','wb')\n",
    "#pickle.dump(my_dict,file)\n",
    "#file.close()\n",
    "\n",
    "file=open('tokens-dict.pkl','rb')\n",
    "posting_list=pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "#documents dictionary:\n",
    "#file=open('doc-dict.pkl','wb')\n",
    "#pickle.dump(documents,file)\n",
    "#file.close()\n",
    "\n",
    "file=open('doc-dict.pkl','rb')\n",
    "documents_list=pickle.load(file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "23a56b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to refine the query\n",
    "def preprocess(query):\n",
    "    query_tokens=nltk.word_tokenize(query)\n",
    "    query_tokens=[word.lower() for word in query_tokens] \n",
    "    query_tokens=[word.translate(str.maketrans('','',string.punctuation)) for word in query_tokens]\n",
    "    query_tokens=[word.translate(str.maketrans('','','1234567890')) for word in query_tokens]\n",
    "        \n",
    "    query_tokens=[word for word in query_tokens if word!='']\n",
    "\n",
    "    query_tokens =[word.lower() for word in query_tokens if word not in stop_words]   #lowercase\n",
    "        \n",
    "\n",
    "    #removal of non-ascii characters\n",
    "    for i in range(0,len(query_tokens)):\n",
    "        temp=query_tokens[i].encode('ascii','ignore')\n",
    "        query_tokens[i]=temp.decode()\n",
    "\n",
    "    #stemming\n",
    "    ps=PorterStemmer()\n",
    "    query_tokens=[ps.stem(word) for word in query_tokens]\n",
    "    \n",
    "    query_tokens=[word for word in query_tokens if word in posting_list]\n",
    "    return query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "15a8ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating term requency dictionary\n",
    "tf_dict={}\n",
    "\n",
    "for key in documents_list:\n",
    "    for word in documents_list[key]:\n",
    "        if word not in tf_dict:\n",
    "            tf_dict[word]={}\n",
    "        else:\n",
    "            if key not in tf_dict[word]:\n",
    "                tf_dict[word][key]=1\n",
    "            else:\n",
    "                tf_dict[word][key]=tf_dict[word][key]+1\n",
    "\n",
    "for word in tf_dict:\n",
    "    for doc_id in tf_dict[word]:\n",
    "        tf_dict[word][doc_id]=tf_dict[word][doc_id]/len(documents_list[doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eee2cb",
   "metadata": {},
   "source": [
    "# BOOLEAN RETRIEVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d77f0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOOLEAN RETRIEVAL MODEL\n",
    "def boolean_retrieval_func(query):\n",
    "    query_tokens=preprocess(query)\n",
    "    \n",
    "    temp_set1=posting_list[query_tokens[0]]     #giving reference of a set to temp_set1\n",
    "    for i in range(1,len(query_tokens)):\n",
    "        temp_set2=posting_list[query_tokens[i]]\n",
    "        temp_set1=temp_set1 & temp_set2         #performing intersection of sets(very fast operation)\n",
    "    \n",
    "    top_10=[]\n",
    "    for doc_id in temp_set1:\n",
    "        score=0\n",
    "        for word in query_tokens:\n",
    "            if doc_id not in tf_dict[word]:\n",
    "                continue\n",
    "            score+=tf_dict[word][doc_id]\n",
    "            \n",
    "        top_10.append((score,doc_id))\n",
    "        \n",
    "    top_10.sort(reverse=True)\n",
    "    res=[]\n",
    "    for i in range(min(10,len(top_10))):\n",
    "        res.append(top_10[i][1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5379eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#creating idf dictionary\n",
    "idf_dict={}\n",
    "n_docs=8635\n",
    "\n",
    "for word in posting_list:\n",
    "    n_docs_curr_word=len(posting_list[word])\n",
    "    idf_dict[word]=math.log(n_docs/n_docs_curr_word)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4e3d71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict={}    #dictionary to store words and their correspondihng index(to be used in calculation of cosine similarity)\n",
    "i=0\n",
    "for word in posting_list:\n",
    "    vocab_dict[word]=i\n",
    "    i=i+1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bdf1357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2243.5438332368267\n"
     ]
    }
   ],
   "source": [
    "#average length of documents which is required in BM25\n",
    "L=0\n",
    "for doc in documents_list:\n",
    "    L=L+len(documents_list[doc])\n",
    "    #print(len(doc))\n",
    "    \n",
    "L=L/len(documents_list)\n",
    "\n",
    "len(documents_list)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5685f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.47 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#storing unique words present in each document\n",
    "doc_unique = {}\n",
    "\n",
    "for doc in documents_list : \n",
    "    doc_unique[doc] = set()\n",
    "    for word in documents_list[doc] :\n",
    "        doc_unique[doc].add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff09734",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c91c4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab_len=len(posting_list)  #no of unique words in all the documents\n",
    "A=np.zeros((vocab_len))\n",
    "\n",
    "#function to compute tf-idf value of each document corresponding to given query\n",
    "def tfidf(query):\n",
    "    cosine_similarity=[]\n",
    "    query_tokens= preprocess(query)\n",
    "    B=np.zeros((vocab_len))\n",
    "    tf_query={}\n",
    "    for word in query_tokens:\n",
    "        tf_query[word]=0\n",
    "    for word in query_tokens:\n",
    "        tf_query[word]+=1\n",
    "    for word in query_tokens:\n",
    "        tf_query[word]/=len(query_tokens)\n",
    "        \n",
    "    for word in query_tokens:\n",
    "        word_index=vocab_dict[word]\n",
    "        B[word_index]=tf_query[word]*idf_dict[word]\n",
    "        \n",
    "    normB=np.linalg.norm(B)\n",
    "    documents_imp = set()\n",
    "    for word in query_tokens :\n",
    "        for doc_id in posting_list[word]:\n",
    "            documents_imp.add(doc_id)\n",
    "    \n",
    "    \n",
    "    for doc_id in documents_imp:\n",
    "        temp = np.zeros((vocab_len))\n",
    "        for word in doc_unique[doc_id]:\n",
    "            if doc_id not in tf_dict[word]:\n",
    "                continue\n",
    "            tf_value=tf_dict[word][doc_id]\n",
    "            word_index=vocab_dict[word]\n",
    "            temp[word_index]=idf_dict[word]*tf_value\n",
    "        normTemp=np.linalg.norm(temp)\n",
    "        cosine_score=np.dot(temp,B)/(normTemp*normB)\n",
    "        cosine_similarity.append((cosine_score,doc_id))\n",
    "        \n",
    "    cosine_similarity.sort(reverse=True)\n",
    "    ans=[]\n",
    "    n=len(cosine_similarity)\n",
    "    for i in range(min(10,len(cosine_similarity))):\n",
    "        ans.append(cosine_similarity[i][1])\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "24825307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C00030', 'S00003', 'C00743', 'C00015', 'S00407', 'C00296', 'C00551', 'C00021', 'C00762', 'C00678']\n",
      "CPU times: total: 42.8 s\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans=tfidf(\"Google made a wide deployment of standards\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ede04",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "af363162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate bm25 score of each document corresponding to given query\n",
    "def BM25(query,k=2,b=0.75):\n",
    "    query_tokens=preprocess(query)\n",
    "    result=[]\n",
    "    N=len(documents_list)\n",
    "    documents_imp = set()\n",
    "    for word in query_tokens :\n",
    "        for doc_id in posting_list[word]:\n",
    "            documents_imp.add(doc_id)    #document_imp denotes the documents in which current token of query is present\n",
    "    for doc_id in documents_imp:\n",
    "        doc_score=0\n",
    "        for word in query_tokens:\n",
    "            if doc_id not in tf_dict[word]:\n",
    "                continue\n",
    "            x_idf=(N - len(posting_list[word]) + 0.5)/(len(posting_list[word])+0.5)\n",
    "            x_idf=math.log(x_idf)\n",
    "            \n",
    "            second_term=(tf_dict[word][doc_id]*len(documents_list[doc_id]) * (k+1) )/(tf_dict[word][doc_id]*len(documents_list[doc_id]) + k*(1-b+b*len(documents_list[doc_id])/L))\n",
    "            prod=x_idf*second_term\n",
    "            doc_score+=prod\n",
    "            \n",
    "        result.append((doc_score,doc_id))\n",
    "        \n",
    "    result.sort(reverse=True)\n",
    "    ans=[]\n",
    "    for i in range(min(10,len(result))):\n",
    "        ans.append(result[i][1])\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "299a31e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C00017', 'C00519', 'C00345', 'M00003', 'S00180', 'C00387', 'C00795', 'D00828', 'S00063', 'S00073']\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 59.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#CHECKING SOME QUERIES FOR RESULTS\n",
    "ans=BM25(\"Internet of things describes physical objects\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d6dc0473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D00985',\n",
       " 'D00488',\n",
       " 'D00101',\n",
       " 'D00549',\n",
       " 'D00681',\n",
       " 'D00171',\n",
       " 'D00428',\n",
       " 'D00886',\n",
       " 'D00258',\n",
       " 'D00141']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHECKING SOME RESULTS\n",
    "boolean_retrieval_func(\"replacing the absent hormones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "df634bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' TO GENERATE QRELS\n",
    "file_read=open('queries.txt')\n",
    "a=file_read.readlines()\n",
    "file_write=open('qrels.txt','a')\n",
    "for line in a:\n",
    "    qid=re.sub('\\t.*','',line)\n",
    "    qid=re.sub('\\n','',qid)\n",
    "    stringh=re.sub('Q[0-9][0-9]\\t','',line)\n",
    "    docs_bm=BM25(stringh)\n",
    "    for doc_id in docs_bm:\n",
    "        file_write.write(qid+', 1, '+doc_id+', 1\\n')\n",
    "    \n",
    "file_read.close()\n",
    "file_write.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532554e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
